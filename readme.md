# ğŸ“„ Resume Bullet Rewriting with LLaMA 3.1

This project demonstrates how to rewrite weak or generic resume bullet points using various prompting strategies with the `meta-llama/Meta-Llama-3.1-8B-Instruct` model via Hugging Face's `InferenceClient`.

---

## ğŸ”§ Tools & Models Used

- **Model**: `meta-llama/Meta-Llama-3.1-8B-Instruct`
- **Interface**: Hugging Face InferenceClient API
- **Language**: Python
- **Environment**: `.env` file for API key (`HF_TOKEN`)
- **Notebooks**: 10 Jupyter notebooks (each implementing a different prompting strategy)
- **Prompts**: 10 `.txt` files under the `prompt/` directory, each containing a unique prompting template

---

## ğŸ§  Prompting Strategies

Each notebook demonstrates a distinct prompt engineering technique:

1. **Direct Instruction Prompting** â€“ Clear and concise commands
2. **Role-playing Prompting** â€“ Model behaves as a recruiter or resume expert
3. **Few-shot Prompting** â€“ Learning from given examples
4. **Chain-of-Thought Prompting** â€“ Step-by-step reasoning before rewriting
5. **Keyword-Focused Prompting** â€“ Emphasis on industry-specific terminology
6. **Metrics-Focused Prompting** â€“ Highlights quantifiable outcomes
7. **Simplified Prompting** â€“ Minimal instructions to observe model default behavior
8. **Negative Example Avoidance** â€“ Includes poor examples to avoid
9. **Persona Prompting** â€“ Role-specific guidance (e.g., Data Scientist)
10. **Socratic Prompting** â€“ Uses Q&A-style reflection before rewriting

> **Bonus:** Some notebooks explore **Multi-turn Prompting** for self-refinement.

---

## ğŸ† Best Performing Prompt

After empirical comparison across strategies, **Chain-of-Thought Prompting** stood out due to:

- More structured and achievement-focused rewrites  
- Logical reasoning that included measurable impact  
- Minimal vague or generic language  

This approach struck a strong balance between **creativity**, **clarity**, and **effectiveness**.

---

## âš ï¸ Challenges Faced

- **Prompt Sensitivity**: Minor changes in text caused major changes in output
- **Rate Limiting**: Introduced `time.sleep()` to avoid API throttling
- **Token Limitations**: Required prompt trimming and formatting control
- **Dynamic Prompting**: Used placeholder `{{bullet}}` to inject resume points into templates across notebooks

---

## ğŸ“ File Structure

```bash
â”œâ”€â”€ prompt/
â”‚   â”œâ”€â”€ direct_instruction.txt
â”‚   â”œâ”€â”€ role_playing.txt
â”‚   â”œâ”€â”€ few_shot.txt
â”‚   â”œâ”€â”€ chainofthought.txt
â”‚   â”œâ”€â”€ keyword_focused.txt
â”‚   â”œâ”€â”€ metrics_focused.txt
â”‚   â”œâ”€â”€ simplified.txt
â”‚   â”œâ”€â”€ negative_examples.txt
â”‚   â”œâ”€â”€ persona_data_science.txt
â”‚   â”œâ”€â”€ socratic.txt
â”‚
â”œâ”€â”€ direct_instruction.ipynb
â”œâ”€â”€ role_playing.ipynb
â”œâ”€â”€ few_shot.ipynb
â”œâ”€â”€ chainofthought.ipynb
â”œâ”€â”€ keyword_focused.ipynb
â”œâ”€â”€ metrics_focused.ipynb
â”œâ”€â”€ simplified.ipynb
â”œâ”€â”€ negative_examples.ipynb
â”œâ”€â”€ persona_prompt.ipynb
â”œâ”€â”€ socratic_prompt.ipynb
â”œâ”€â”€ bullet.txt
â”œâ”€â”€ .env
